{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "string=\"\"\"Paragraphs are the building blocks of papers. Many students define paragraphs in terms of length: a paragraph is a group of at least five sentences, a paragraph is half a page long, etc. In reality, though, the unity and coherence of ideas among sentences is what constitutes a paragraph. A paragraph is defined as “a group of sentences or a single sentence that forms a unit” (Lunsford and Connors 116). Length and appearance do not determine whether a section in a paper is a paragraph. For instance, in some styles of writing, particularly journalistic styles, a paragraph can be just one sentence long. Ultimately, a paragraph is a sentence or group of sentences that support one main idea. In this handout, we will refer to this as the “controlling idea,” because it controls what happens in the rest of the paragraph.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paragraphs are the building blocks of papers.',\n",
       " 'Many students define paragraphs in terms of length: a paragraph is a group of at least five sentences, a paragraph is half a page long, etc.',\n",
       " 'In reality, though, the unity and coherence of ideas among sentences is what constitutes a paragraph.',\n",
       " 'A paragraph is defined as “a group of sentences or a single sentence that forms a unit” (Lunsford and Connors 116).',\n",
       " 'Length and appearance do not determine whether a section in a paper is a paragraph.',\n",
       " 'For instance, in some styles of writing, particularly journalistic styles, a paragraph can be just one sentence long.',\n",
       " 'Ultimately, a paragraph is a sentence or group of sentences that support one main idea.',\n",
       " 'In this handout, we will refer to this as the “controlling idea,” because it controls what happens in the rest of the paragraph.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_tokens=sent_tokenize(string)\n",
    "s_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paragraphs',\n",
       " 'are',\n",
       " 'the',\n",
       " 'building',\n",
       " 'blocks',\n",
       " 'of',\n",
       " 'papers',\n",
       " '.',\n",
       " 'Many',\n",
       " 'students',\n",
       " 'define',\n",
       " 'paragraphs',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'length',\n",
       " ':',\n",
       " 'a',\n",
       " 'paragraph',\n",
       " 'is',\n",
       " 'a',\n",
       " 'group',\n",
       " 'of',\n",
       " 'at',\n",
       " 'least',\n",
       " 'five',\n",
       " 'sentences',\n",
       " ',',\n",
       " 'a',\n",
       " 'paragraph',\n",
       " 'is',\n",
       " 'half',\n",
       " 'a',\n",
       " 'page',\n",
       " 'long',\n",
       " ',',\n",
       " 'etc',\n",
       " '.',\n",
       " 'In',\n",
       " 'reality',\n",
       " ',',\n",
       " 'though',\n",
       " ',',\n",
       " 'the',\n",
       " 'unity',\n",
       " 'and',\n",
       " 'coherence',\n",
       " 'of',\n",
       " 'ideas',\n",
       " 'among',\n",
       " 'sentences',\n",
       " 'is',\n",
       " 'what',\n",
       " 'constitutes',\n",
       " 'a',\n",
       " 'paragraph',\n",
       " '.',\n",
       " 'A',\n",
       " 'paragraph',\n",
       " 'is',\n",
       " 'defined',\n",
       " 'as',\n",
       " '“',\n",
       " 'a',\n",
       " 'group',\n",
       " 'of',\n",
       " 'sentences',\n",
       " 'or',\n",
       " 'a',\n",
       " 'single',\n",
       " 'sentence',\n",
       " 'that',\n",
       " 'forms',\n",
       " 'a',\n",
       " 'unit',\n",
       " '”',\n",
       " '(',\n",
       " 'Lunsford',\n",
       " 'and',\n",
       " 'Connors',\n",
       " '116',\n",
       " ')',\n",
       " '.',\n",
       " 'Length',\n",
       " 'and',\n",
       " 'appearance',\n",
       " 'do',\n",
       " 'not',\n",
       " 'determine',\n",
       " 'whether',\n",
       " 'a',\n",
       " 'section',\n",
       " 'in',\n",
       " 'a',\n",
       " 'paper',\n",
       " 'is',\n",
       " 'a',\n",
       " 'paragraph',\n",
       " '.',\n",
       " 'For',\n",
       " 'instance',\n",
       " ',',\n",
       " 'in',\n",
       " 'some',\n",
       " 'styles',\n",
       " 'of',\n",
       " 'writing',\n",
       " ',',\n",
       " 'particularly',\n",
       " 'journalistic',\n",
       " 'styles',\n",
       " ',',\n",
       " 'a',\n",
       " 'paragraph',\n",
       " 'can',\n",
       " 'be',\n",
       " 'just',\n",
       " 'one',\n",
       " 'sentence',\n",
       " 'long',\n",
       " '.',\n",
       " 'Ultimately',\n",
       " ',',\n",
       " 'a',\n",
       " 'paragraph',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'or',\n",
       " 'group',\n",
       " 'of',\n",
       " 'sentences',\n",
       " 'that',\n",
       " 'support',\n",
       " 'one',\n",
       " 'main',\n",
       " 'idea',\n",
       " '.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'handout',\n",
       " ',',\n",
       " 'we',\n",
       " 'will',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'this',\n",
       " 'as',\n",
       " 'the',\n",
       " '“',\n",
       " 'controlling',\n",
       " 'idea',\n",
       " ',',\n",
       " '”',\n",
       " 'because',\n",
       " 'it',\n",
       " 'controls',\n",
       " 'what',\n",
       " 'happens',\n",
       " 'in',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'paragraph',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_tokens=word_tokenize(string)\n",
    "w_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenize_text=RegexpTokenizer(r\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paragraphs',\n",
       " 'are',\n",
       " 'the',\n",
       " 'building',\n",
       " 'blocks',\n",
       " 'of',\n",
       " 'papers',\n",
       " 'Many',\n",
       " 'students',\n",
       " 'define',\n",
       " 'paragraphs',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'length',\n",
       " 'a',\n",
       " 'paragraph',\n",
       " 'is',\n",
       " 'a',\n",
       " 'group',\n",
       " 'of',\n",
       " 'at',\n",
       " 'least',\n",
       " 'five',\n",
       " 'sentences',\n",
       " 'a',\n",
       " 'paragraph',\n",
       " 'is',\n",
       " 'half',\n",
       " 'a',\n",
       " 'page',\n",
       " 'long',\n",
       " 'etc',\n",
       " 'In',\n",
       " 'reality',\n",
       " 'though',\n",
       " 'the',\n",
       " 'unity',\n",
       " 'and',\n",
       " 'coherence',\n",
       " 'of',\n",
       " 'ideas',\n",
       " 'among',\n",
       " 'sentences',\n",
       " 'is',\n",
       " 'what',\n",
       " 'constitutes',\n",
       " 'a',\n",
       " 'paragraph',\n",
       " 'A',\n",
       " 'paragraph',\n",
       " 'is',\n",
       " 'defined',\n",
       " 'as',\n",
       " 'a',\n",
       " 'group',\n",
       " 'of',\n",
       " 'sentences',\n",
       " 'or',\n",
       " 'a',\n",
       " 'single',\n",
       " 'sentence',\n",
       " 'that',\n",
       " 'forms',\n",
       " 'a',\n",
       " 'unit',\n",
       " 'Lunsford',\n",
       " 'and',\n",
       " 'Connors',\n",
       " '116',\n",
       " 'Length',\n",
       " 'and',\n",
       " 'appearance',\n",
       " 'do',\n",
       " 'not',\n",
       " 'determine',\n",
       " 'whether',\n",
       " 'a',\n",
       " 'section',\n",
       " 'in',\n",
       " 'a',\n",
       " 'paper',\n",
       " 'is',\n",
       " 'a',\n",
       " 'paragraph',\n",
       " 'For',\n",
       " 'instance',\n",
       " 'in',\n",
       " 'some',\n",
       " 'styles',\n",
       " 'of',\n",
       " 'writing',\n",
       " 'particularly',\n",
       " 'journalistic',\n",
       " 'styles',\n",
       " 'a',\n",
       " 'paragraph',\n",
       " 'can',\n",
       " 'be',\n",
       " 'just',\n",
       " 'one',\n",
       " 'sentence',\n",
       " 'long',\n",
       " 'Ultimately',\n",
       " 'a',\n",
       " 'paragraph',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'or',\n",
       " 'group',\n",
       " 'of',\n",
       " 'sentences',\n",
       " 'that',\n",
       " 'support',\n",
       " 'one',\n",
       " 'main',\n",
       " 'idea',\n",
       " 'In',\n",
       " 'this',\n",
       " 'handout',\n",
       " 'we',\n",
       " 'will',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'this',\n",
       " 'as',\n",
       " 'the',\n",
       " 'controlling',\n",
       " 'idea',\n",
       " 'because',\n",
       " 'it',\n",
       " 'controls',\n",
       " 'what',\n",
       " 'happens',\n",
       " 'in',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'paragraph']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "without_punctuation_marks=tokenize_text.tokenize(string)\n",
    "without_punctuation_marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Pankaj kumar\n",
      "[nltk_data]     shah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_word_list=set(stopwords.words(\"english\"))\n",
    "stop_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paragraphs',\n",
       " 'building',\n",
       " 'blocks',\n",
       " 'papers',\n",
       " 'Many',\n",
       " 'students',\n",
       " 'define',\n",
       " 'paragraphs',\n",
       " 'terms',\n",
       " 'length',\n",
       " 'paragraph',\n",
       " 'group',\n",
       " 'least',\n",
       " 'five',\n",
       " 'sentences',\n",
       " 'paragraph',\n",
       " 'half',\n",
       " 'page',\n",
       " 'long',\n",
       " 'etc']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string=\"Paragraphs are the building blocks of papers. Many students define paragraphs in terms of length: a paragraph is a group of at least five sentences, a paragraph is half a page long, etc.\"\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "punctuation_remover=RegexpTokenizer(r\"\\w+\")\n",
    "tokens=punctuation_remover.tokenize(string)\n",
    "\n",
    "new_tokens=[]\n",
    "for i in tokens:\n",
    "    if i not in stop_word_list:new_tokens.append(i)\n",
    "        \n",
    "new_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paragraph',\n",
       " 'build',\n",
       " 'block',\n",
       " 'paper',\n",
       " 'mani',\n",
       " 'student',\n",
       " 'defin',\n",
       " 'paragraph',\n",
       " 'term',\n",
       " 'length',\n",
       " 'paragraph',\n",
       " 'group',\n",
       " 'least',\n",
       " 'five',\n",
       " 'sentenc',\n",
       " 'paragraph',\n",
       " 'half',\n",
       " 'page',\n",
       " 'long',\n",
       " 'etc']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "stem=[stemmer.stem(i) for i in new_tokens]\n",
    "stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paragraph build block paper mani student defin paragraph term length paragraph group least five sentenc paragraph half page long etc '"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mreging the list to create string\n",
    "string=\"\"\n",
    "for i in stem:\n",
    "    string=string+i+\" \"\n",
    "\n",
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paragraphs',\n",
       " 'build',\n",
       " 'block',\n",
       " 'paper',\n",
       " 'Many',\n",
       " 'students',\n",
       " 'define',\n",
       " 'paragraph',\n",
       " 'term',\n",
       " 'length',\n",
       " 'paragraph',\n",
       " 'group',\n",
       " 'least',\n",
       " 'five',\n",
       " 'sentence',\n",
       " 'paragraph',\n",
       " 'half',\n",
       " 'page',\n",
       " 'long',\n",
       " 'etc']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "lemma=[lemmatizer.lemmatize(i,pos=\"v\") for i in new_tokens]\n",
    "lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>stem</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paragraphs</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Paragraphs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>building</td>\n",
       "      <td>build</td>\n",
       "      <td>build</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blocks</td>\n",
       "      <td>block</td>\n",
       "      <td>block</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>papers</td>\n",
       "      <td>paper</td>\n",
       "      <td>paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Many</td>\n",
       "      <td>mani</td>\n",
       "      <td>Many</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>students</td>\n",
       "      <td>student</td>\n",
       "      <td>students</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>define</td>\n",
       "      <td>defin</td>\n",
       "      <td>define</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>paragraphs</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>paragraph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>terms</td>\n",
       "      <td>term</td>\n",
       "      <td>term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>length</td>\n",
       "      <td>length</td>\n",
       "      <td>length</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>paragraph</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>paragraph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>group</td>\n",
       "      <td>group</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>least</td>\n",
       "      <td>least</td>\n",
       "      <td>least</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>five</td>\n",
       "      <td>five</td>\n",
       "      <td>five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sentences</td>\n",
       "      <td>sentenc</td>\n",
       "      <td>sentence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>paragraph</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>paragraph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>half</td>\n",
       "      <td>half</td>\n",
       "      <td>half</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>page</td>\n",
       "      <td>page</td>\n",
       "      <td>page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>long</td>\n",
       "      <td>long</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>etc</td>\n",
       "      <td>etc</td>\n",
       "      <td>etc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tokens       stem       lemma\n",
       "0   Paragraphs  paragraph  Paragraphs\n",
       "1     building      build       build\n",
       "2       blocks      block       block\n",
       "3       papers      paper       paper\n",
       "4         Many       mani        Many\n",
       "5     students    student    students\n",
       "6       define      defin      define\n",
       "7   paragraphs  paragraph   paragraph\n",
       "8        terms       term        term\n",
       "9       length     length      length\n",
       "10   paragraph  paragraph   paragraph\n",
       "11       group      group       group\n",
       "12       least      least       least\n",
       "13        five       five        five\n",
       "14   sentences    sentenc    sentence\n",
       "15   paragraph  paragraph   paragraph\n",
       "16        half       half        half\n",
       "17        page       page        page\n",
       "18        long       long        long\n",
       "19         etc        etc         etc"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df1=pd.DataFrame()\n",
    "df1[\"tokens\"]=new_tokens\n",
    "df1[\"stem\"]=stem\n",
    "df1[\"lemma\"]=lemma\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here begins the great works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embadding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training Word2Vec Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Its time to make dirty your hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>sentiment_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.245025e+18</td>\n",
       "      <td>Happy #MothersDay to all you amazing mothers o...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>BeenXXPired</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245759e+18</td>\n",
       "      <td>Happy Mothers Day Mum - I'm sorry I can't be t...</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>FestiveFeeling</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.246087e+18</td>\n",
       "      <td>Happy mothers day To all This doing a mothers ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>KrisAllenSak</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.244803e+18</td>\n",
       "      <td>Happy mothers day to this beautiful woman...ro...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Queenuchee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.244876e+18</td>\n",
       "      <td>Remembering the 3 most amazing ladies who made...</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>brittan17446794</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                      original_text lang  \\\n",
       "0  1.245025e+18  Happy #MothersDay to all you amazing mothers o...   en   \n",
       "1  1.245759e+18  Happy Mothers Day Mum - I'm sorry I can't be t...   en   \n",
       "2  1.246087e+18  Happy mothers day To all This doing a mothers ...   en   \n",
       "3  1.244803e+18  Happy mothers day to this beautiful woman...ro...   en   \n",
       "4  1.244876e+18  Remembering the 3 most amazing ladies who made...   en   \n",
       "\n",
       "  retweet_count  original_author  sentiment_class  \n",
       "0             0      BeenXXPired                0  \n",
       "1             1   FestiveFeeling                0  \n",
       "2             0     KrisAllenSak               -1  \n",
       "3             0       Queenuchee                0  \n",
       "4             0  brittan17446794               -1  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(r\"D:\\machine_learning\\data\\dataset\\train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>sentiment_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Happy #MothersDay to all you amazing mothers o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Happy Mothers Day Mum - I'm sorry I can't be t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Happy mothers day To all This doing a mothers ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happy mothers day to this beautiful woman...ro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remembering the 3 most amazing ladies who made...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  sentiment_class\n",
       "0  Happy #MothersDay to all you amazing mothers o...                0\n",
       "1  Happy Mothers Day Mum - I'm sorry I can't be t...                0\n",
       "2  Happy mothers day To all This doing a mothers ...               -1\n",
       "3  Happy mothers day to this beautiful woman...ro...                0\n",
       "4  Remembering the 3 most amazing ladies who made...               -1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame()\n",
    "df=data[[\"original_text\",\"sentiment_class\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3235"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating empty list\n",
    "tweet_list=list()\n",
    "\n",
    "\n",
    "for line in df[\"original_text\"]:\n",
    "    #removing puctuation marks\n",
    "    punctuation_remover=RegexpTokenizer(r\"\\w+\")\n",
    "    tokens=punctuation_remover.tokenize(line)\n",
    "    \n",
    "    #converting all tokens into lowercase\n",
    "    tokens=[i.lower() for i in tokens]\n",
    "    \n",
    "    #removing stopwords\n",
    "    stop_words=set(stopwords.words(\"english\"))\n",
    "    \n",
    "    words=[i for i in tokens if not i in stop_words]\n",
    "    \n",
    "    #appending the tweet list\n",
    "    tweet_list.append(words)\n",
    "    \n",
    "len(tweet_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-win_amd64.whl (24.2 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in d:\\anaconda\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in d:\\anaconda\\lib\\site-packages (from gensim) (1.14.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-2.0.0.tar.gz (103 kB)\n",
      "Collecting Cython==0.29.14\n",
      "  Downloading Cython-0.29.14-cp37-cp37m-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in d:\\anaconda\\lib\\site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: boto in d:\\anaconda\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.13.19.tar.gz (97 kB)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Collecting botocore<1.17.0,>=1.16.19\n",
      "  Downloading botocore-1.16.19-py2.py3-none-any.whl (6.2 MB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in d:\\anaconda\\lib\\site-packages (from botocore<1.17.0,>=1.16.19->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Building wheels for collected packages: smart-open, boto3\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-2.0.0-py3-none-any.whl size=101346 sha256=c28e82cfa34befc00ce5933745440e25bfa7f321cf5468624c441047b195c194\n",
      "  Stored in directory: c:\\users\\pankaj kumar shah\\appdata\\local\\pip\\cache\\wheels\\bb\\1c\\9c\\412ec03f6d5ac7d41f4b965bde3fc0d1bd201da5ba3e2636de\n",
      "  Building wheel for boto3 (setup.py): started\n",
      "  Building wheel for boto3 (setup.py): finished with status 'done'\n",
      "  Created wheel for boto3: filename=boto3-1.13.19-py2.py3-none-any.whl size=127658 sha256=6db9e26416db7bd12466c5a1821b8d6a7972be343e70d59ceeb0210c39e839b0\n",
      "  Stored in directory: c:\\users\\pankaj kumar shah\\appdata\\local\\pip\\cache\\wheels\\bf\\3a\\84\\9561839c691a06e6f4db54b26aea99f884be1da98dc5cb4222\n",
      "Successfully built smart-open boto3\n",
      "Installing collected packages: docutils, jmespath, botocore, s3transfer, boto3, smart-open, Cython, gensim\n",
      "  Attempting uninstall: docutils\n",
      "    Found existing installation: docutils 0.16\n",
      "    Uninstalling docutils-0.16:\n",
      "      Successfully uninstalled docutils-0.16\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.15\n",
      "    Uninstalling Cython-0.29.15:\n",
      "      Successfully uninstalled Cython-0.29.15\n",
      "Successfully installed Cython-0.29.14 boto3-1.13.19 botocore-1.16.19 docutils-0.15.2 gensim-3.8.3 jmespath-0.10.0 s3transfer-0.3.3 smart-open-2.0.0\n"
     ]
    }
   ],
   "source": [
    "#training the model using gensim\n",
    "Embdding_dim=100\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1377"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "model=gensim.models.Word2Vec(sentences=tweet_list,size=Embdding_dim,workers=4)\n",
    "\n",
    "#vocabulary size\n",
    "words=list(model.wv.vocab)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('beautiful', 0.9997763633728027),\n",
       " ('mum', 0.9995499849319458),\n",
       " ('special', 0.9993190765380859),\n",
       " ('everyone', 0.9991933107376099),\n",
       " ('step', 0.9989210367202759),\n",
       " ('one', 0.9988945722579956),\n",
       " ('lovely', 0.9988877773284912),\n",
       " ('hope', 0.9988337755203247),\n",
       " ('heavenly', 0.9988223314285278),\n",
       " ('wishing', 0.9987958073616028)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding most similar words\n",
    "model.wv.most_similar(\"amazing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('celebrate', 0.9998244047164917),\n",
       " ('wife', 0.9998244047164917),\n",
       " ('big', 0.9998224973678589),\n",
       " ('little', 0.9998025894165039),\n",
       " ('people', 0.9997915625572205),\n",
       " ('moms', 0.9997851848602295),\n",
       " ('mine', 0.9997806549072266),\n",
       " ('need', 0.9997797608375549),\n",
       " ('come', 0.9997796416282654),\n",
       " ('really', 0.9997706413269043)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"sorry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('every', 0.9965646862983704),\n",
       " ('around', 0.9965159893035889),\n",
       " ('working', 0.9965154528617859),\n",
       " ('great', 0.9964962005615234),\n",
       " ('fantastic', 0.9964791536331177),\n",
       " ('want', 0.9964755177497864),\n",
       " ('hard', 0.9964421391487122),\n",
       " ('especially', 0.9964317679405212),\n",
       " ('best', 0.9963839650154114),\n",
       " ('figures', 0.9963585734367371)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#perrforming some mathematics on cosine \n",
    "model.wv.most_similar_cosmul(positive=[\"amazing\",\"sorry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'women'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"man women car\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "its funny beacuse our data is not trained efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
