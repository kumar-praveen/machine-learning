{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cricket=\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and fishery rihgts at once. He was the more ready to do this becuase the rights had becom much less valuable, and he had indeed the vaguest idea where the wood and river in quedtion were.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'determined',\n",
       " 'to',\n",
       " 'drop',\n",
       " 'his',\n",
       " 'litigation',\n",
       " 'with',\n",
       " 'the',\n",
       " 'monastry',\n",
       " ',',\n",
       " 'and',\n",
       " 'relinguish',\n",
       " 'his',\n",
       " 'claims',\n",
       " 'to',\n",
       " 'the',\n",
       " 'wood-cuting',\n",
       " 'and',\n",
       " 'fishery',\n",
       " 'rihgts',\n",
       " 'at',\n",
       " 'once',\n",
       " '.',\n",
       " 'He',\n",
       " 'was',\n",
       " 'the',\n",
       " 'more',\n",
       " 'ready',\n",
       " 'to',\n",
       " 'do',\n",
       " 'this',\n",
       " 'becuase',\n",
       " 'the',\n",
       " 'rights',\n",
       " 'had',\n",
       " 'becom',\n",
       " 'much',\n",
       " 'less',\n",
       " 'valuable',\n",
       " ',',\n",
       " 'and',\n",
       " 'he',\n",
       " 'had',\n",
       " 'indeed',\n",
       " 'the',\n",
       " 'vaguest',\n",
       " 'idea',\n",
       " 'where',\n",
       " 'the',\n",
       " 'wood',\n",
       " 'and',\n",
       " 'river',\n",
       " 'in',\n",
       " 'quedtion',\n",
       " 'were',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=word_tokenize(cricket)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist=FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 56)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens),len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 6, 'and': 4, 'to': 3, 'He': 2, 'his': 2, ',': 2, '.': 2, 'had': 2, 'determined': 1, 'drop': 1, ...})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in tokens:\n",
    "    fdist[i]=fdist[i]+1\n",
    "    \n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 6),\n",
       " ('and', 4),\n",
       " ('to', 3),\n",
       " ('He', 2),\n",
       " ('his', 2),\n",
       " (',', 2),\n",
       " ('.', 2),\n",
       " ('had', 2),\n",
       " ('determined', 1),\n",
       " ('drop', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10=fdist.most_common(10)\n",
    "top_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating bigram and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'determined'),\n",
       " ('determined', 'to'),\n",
       " ('to', 'drop'),\n",
       " ('drop', 'his'),\n",
       " ('his', 'litigation'),\n",
       " ('litigation', 'with'),\n",
       " ('with', 'the'),\n",
       " ('the', 'monastry'),\n",
       " ('monastry', ','),\n",
       " (',', 'and'),\n",
       " ('and', 'relinguish'),\n",
       " ('relinguish', 'his'),\n",
       " ('his', 'claims'),\n",
       " ('claims', 'to'),\n",
       " ('to', 'the'),\n",
       " ('the', 'wood-cuting'),\n",
       " ('wood-cuting', 'and'),\n",
       " ('and', 'fishery'),\n",
       " ('fishery', 'rihgts'),\n",
       " ('rihgts', 'at'),\n",
       " ('at', 'once'),\n",
       " ('once', '.'),\n",
       " ('.', 'He'),\n",
       " ('He', 'was'),\n",
       " ('was', 'the'),\n",
       " ('the', 'more'),\n",
       " ('more', 'ready'),\n",
       " ('ready', 'to'),\n",
       " ('to', 'do'),\n",
       " ('do', 'this'),\n",
       " ('this', 'becuase'),\n",
       " ('becuase', 'the'),\n",
       " ('the', 'rights'),\n",
       " ('rights', 'had'),\n",
       " ('had', 'becom'),\n",
       " ('becom', 'much'),\n",
       " ('much', 'less'),\n",
       " ('less', 'valuable'),\n",
       " ('valuable', ','),\n",
       " (',', 'and'),\n",
       " ('and', 'he'),\n",
       " ('he', 'had'),\n",
       " ('had', 'indeed'),\n",
       " ('indeed', 'the'),\n",
       " ('the', 'vaguest'),\n",
       " ('vaguest', 'idea'),\n",
       " ('idea', 'where'),\n",
       " ('where', 'the'),\n",
       " ('the', 'wood'),\n",
       " ('wood', 'and'),\n",
       " ('and', 'river'),\n",
       " ('river', 'in'),\n",
       " ('in', 'quedtion'),\n",
       " ('quedtion', 'were'),\n",
       " ('were', '.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram=nltk.bigrams(tokens)\n",
    "list(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'determined', 'to'),\n",
       " ('determined', 'to', 'drop'),\n",
       " ('to', 'drop', 'his'),\n",
       " ('drop', 'his', 'litigation'),\n",
       " ('his', 'litigation', 'with'),\n",
       " ('litigation', 'with', 'the'),\n",
       " ('with', 'the', 'monastry'),\n",
       " ('the', 'monastry', ','),\n",
       " ('monastry', ',', 'and'),\n",
       " (',', 'and', 'relinguish'),\n",
       " ('and', 'relinguish', 'his'),\n",
       " ('relinguish', 'his', 'claims'),\n",
       " ('his', 'claims', 'to'),\n",
       " ('claims', 'to', 'the'),\n",
       " ('to', 'the', 'wood-cuting'),\n",
       " ('the', 'wood-cuting', 'and'),\n",
       " ('wood-cuting', 'and', 'fishery'),\n",
       " ('and', 'fishery', 'rihgts'),\n",
       " ('fishery', 'rihgts', 'at'),\n",
       " ('rihgts', 'at', 'once'),\n",
       " ('at', 'once', '.'),\n",
       " ('once', '.', 'He'),\n",
       " ('.', 'He', 'was'),\n",
       " ('He', 'was', 'the'),\n",
       " ('was', 'the', 'more'),\n",
       " ('the', 'more', 'ready'),\n",
       " ('more', 'ready', 'to'),\n",
       " ('ready', 'to', 'do'),\n",
       " ('to', 'do', 'this'),\n",
       " ('do', 'this', 'becuase'),\n",
       " ('this', 'becuase', 'the'),\n",
       " ('becuase', 'the', 'rights'),\n",
       " ('the', 'rights', 'had'),\n",
       " ('rights', 'had', 'becom'),\n",
       " ('had', 'becom', 'much'),\n",
       " ('becom', 'much', 'less'),\n",
       " ('much', 'less', 'valuable'),\n",
       " ('less', 'valuable', ','),\n",
       " ('valuable', ',', 'and'),\n",
       " (',', 'and', 'he'),\n",
       " ('and', 'he', 'had'),\n",
       " ('he', 'had', 'indeed'),\n",
       " ('had', 'indeed', 'the'),\n",
       " ('indeed', 'the', 'vaguest'),\n",
       " ('the', 'vaguest', 'idea'),\n",
       " ('vaguest', 'idea', 'where'),\n",
       " ('idea', 'where', 'the'),\n",
       " ('where', 'the', 'wood'),\n",
       " ('the', 'wood', 'and'),\n",
       " ('wood', 'and', 'river'),\n",
       " ('and', 'river', 'in'),\n",
       " ('river', 'in', 'quedtion'),\n",
       " ('in', 'quedtion', 'were'),\n",
       " ('quedtion', 'were', '.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram=nltk.trigrams(tokens)\n",
    "list(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'determined', 'to', 'drop', 'his'),\n",
       " ('determined', 'to', 'drop', 'his', 'litigation'),\n",
       " ('to', 'drop', 'his', 'litigation', 'with'),\n",
       " ('drop', 'his', 'litigation', 'with', 'the'),\n",
       " ('his', 'litigation', 'with', 'the', 'monastry'),\n",
       " ('litigation', 'with', 'the', 'monastry', ','),\n",
       " ('with', 'the', 'monastry', ',', 'and'),\n",
       " ('the', 'monastry', ',', 'and', 'relinguish'),\n",
       " ('monastry', ',', 'and', 'relinguish', 'his'),\n",
       " (',', 'and', 'relinguish', 'his', 'claims'),\n",
       " ('and', 'relinguish', 'his', 'claims', 'to'),\n",
       " ('relinguish', 'his', 'claims', 'to', 'the'),\n",
       " ('his', 'claims', 'to', 'the', 'wood-cuting'),\n",
       " ('claims', 'to', 'the', 'wood-cuting', 'and'),\n",
       " ('to', 'the', 'wood-cuting', 'and', 'fishery'),\n",
       " ('the', 'wood-cuting', 'and', 'fishery', 'rihgts'),\n",
       " ('wood-cuting', 'and', 'fishery', 'rihgts', 'at'),\n",
       " ('and', 'fishery', 'rihgts', 'at', 'once'),\n",
       " ('fishery', 'rihgts', 'at', 'once', '.'),\n",
       " ('rihgts', 'at', 'once', '.', 'He'),\n",
       " ('at', 'once', '.', 'He', 'was'),\n",
       " ('once', '.', 'He', 'was', 'the'),\n",
       " ('.', 'He', 'was', 'the', 'more'),\n",
       " ('He', 'was', 'the', 'more', 'ready'),\n",
       " ('was', 'the', 'more', 'ready', 'to'),\n",
       " ('the', 'more', 'ready', 'to', 'do'),\n",
       " ('more', 'ready', 'to', 'do', 'this'),\n",
       " ('ready', 'to', 'do', 'this', 'becuase'),\n",
       " ('to', 'do', 'this', 'becuase', 'the'),\n",
       " ('do', 'this', 'becuase', 'the', 'rights'),\n",
       " ('this', 'becuase', 'the', 'rights', 'had'),\n",
       " ('becuase', 'the', 'rights', 'had', 'becom'),\n",
       " ('the', 'rights', 'had', 'becom', 'much'),\n",
       " ('rights', 'had', 'becom', 'much', 'less'),\n",
       " ('had', 'becom', 'much', 'less', 'valuable'),\n",
       " ('becom', 'much', 'less', 'valuable', ','),\n",
       " ('much', 'less', 'valuable', ',', 'and'),\n",
       " ('less', 'valuable', ',', 'and', 'he'),\n",
       " ('valuable', ',', 'and', 'he', 'had'),\n",
       " (',', 'and', 'he', 'had', 'indeed'),\n",
       " ('and', 'he', 'had', 'indeed', 'the'),\n",
       " ('he', 'had', 'indeed', 'the', 'vaguest'),\n",
       " ('had', 'indeed', 'the', 'vaguest', 'idea'),\n",
       " ('indeed', 'the', 'vaguest', 'idea', 'where'),\n",
       " ('the', 'vaguest', 'idea', 'where', 'the'),\n",
       " ('vaguest', 'idea', 'where', 'the', 'wood'),\n",
       " ('idea', 'where', 'the', 'wood', 'and'),\n",
       " ('where', 'the', 'wood', 'and', 'river'),\n",
       " ('the', 'wood', 'and', 'river', 'in'),\n",
       " ('wood', 'and', 'river', 'in', 'quedtion'),\n",
       " ('and', 'river', 'in', 'quedtion', 'were'),\n",
       " ('river', 'in', 'quedtion', 'were', '.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.ngrams(tokens,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'determin',\n",
       " 'to',\n",
       " 'drop',\n",
       " 'hi',\n",
       " 'litig',\n",
       " 'with',\n",
       " 'the',\n",
       " 'monastri',\n",
       " ',',\n",
       " 'and',\n",
       " 'relinguish',\n",
       " 'hi',\n",
       " 'claim',\n",
       " 'to',\n",
       " 'the',\n",
       " 'wood-cut',\n",
       " 'and',\n",
       " 'fisheri',\n",
       " 'rihgt',\n",
       " 'at',\n",
       " 'onc',\n",
       " '.',\n",
       " 'He',\n",
       " 'wa',\n",
       " 'the',\n",
       " 'more',\n",
       " 'readi',\n",
       " 'to',\n",
       " 'do',\n",
       " 'thi',\n",
       " 'becuas',\n",
       " 'the',\n",
       " 'right',\n",
       " 'had',\n",
       " 'becom',\n",
       " 'much',\n",
       " 'less',\n",
       " 'valuabl',\n",
       " ',',\n",
       " 'and',\n",
       " 'he',\n",
       " 'had',\n",
       " 'inde',\n",
       " 'the',\n",
       " 'vaguest',\n",
       " 'idea',\n",
       " 'where',\n",
       " 'the',\n",
       " 'wood',\n",
       " 'and',\n",
       " 'river',\n",
       " 'in',\n",
       " 'quedtion',\n",
       " 'were',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem=[pst.stem(i) for i in tokens]\n",
    "stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Pankaj kumar\n",
      "[nltk_data]     shah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemetizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'determined',\n",
       " 'to',\n",
       " 'drop',\n",
       " 'his',\n",
       " 'litigation',\n",
       " 'with',\n",
       " 'the',\n",
       " 'monastry',\n",
       " ',',\n",
       " 'and',\n",
       " 'relinguish',\n",
       " 'his',\n",
       " 'claim',\n",
       " 'to',\n",
       " 'the',\n",
       " 'wood-cuting',\n",
       " 'and',\n",
       " 'fishery',\n",
       " 'rihgts',\n",
       " 'at',\n",
       " 'once',\n",
       " '.',\n",
       " 'He',\n",
       " 'wa',\n",
       " 'the',\n",
       " 'more',\n",
       " 'ready',\n",
       " 'to',\n",
       " 'do',\n",
       " 'this',\n",
       " 'becuase',\n",
       " 'the',\n",
       " 'right',\n",
       " 'had',\n",
       " 'becom',\n",
       " 'much',\n",
       " 'le',\n",
       " 'valuable',\n",
       " ',',\n",
       " 'and',\n",
       " 'he',\n",
       " 'had',\n",
       " 'indeed',\n",
       " 'the',\n",
       " 'vaguest',\n",
       " 'idea',\n",
       " 'where',\n",
       " 'the',\n",
       " 'wood',\n",
       " 'and',\n",
       " 'river',\n",
       " 'in',\n",
       " 'quedtion',\n",
       " 'were',\n",
       " '.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lema=[lemetizer.lemmatize(i) for i in tokens]\n",
    "lema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n"
     ]
    }
   ],
   "source": [
    "words=[\"cats\",\"cacti\",\"geese\"]\n",
    "for i in words:\n",
    "    print(lemetizer.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Pankaj kumar\n",
      "[nltk_data]     shah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('He', 'PRP')],\n",
       " [('determined', 'VBN')],\n",
       " [('to', 'TO')],\n",
       " [('drop', 'NN')],\n",
       " [('his', 'PRP$')],\n",
       " [('litigation', 'NN')],\n",
       " [('with', 'IN')],\n",
       " [('the', 'DT')],\n",
       " [('monastry', 'NN')],\n",
       " [(',', ',')],\n",
       " [('and', 'CC')],\n",
       " [('relinguish', 'NN')],\n",
       " [('his', 'PRP$')],\n",
       " [('claims', 'NNS')],\n",
       " [('to', 'TO')],\n",
       " [('the', 'DT')],\n",
       " [('wood-cuting', 'NN')],\n",
       " [('and', 'CC')],\n",
       " [('fishery', 'NN')],\n",
       " [('rihgts', 'NN')],\n",
       " [('at', 'IN')],\n",
       " [('once', 'RB')],\n",
       " [('.', '.')],\n",
       " [('He', 'PRP')],\n",
       " [('was', 'VBD')],\n",
       " [('the', 'DT')],\n",
       " [('more', 'RBR')],\n",
       " [('ready', 'JJ')],\n",
       " [('to', 'TO')],\n",
       " [('do', 'VB')],\n",
       " [('this', 'DT')],\n",
       " [('becuase', 'NN')],\n",
       " [('the', 'DT')],\n",
       " [('rights', 'NNS')],\n",
       " [('had', 'VBD')],\n",
       " [('becom', 'NN')],\n",
       " [('much', 'JJ')],\n",
       " [('less', 'RBR')],\n",
       " [('valuable', 'JJ')],\n",
       " [(',', ',')],\n",
       " [('and', 'CC')],\n",
       " [('he', 'PRP')],\n",
       " [('had', 'VBD')],\n",
       " [('indeed', 'RB')],\n",
       " [('the', 'DT')],\n",
       " [('vaguest', 'NN')],\n",
       " [('idea', 'NN')],\n",
       " [('where', 'WRB')],\n",
       " [('the', 'DT')],\n",
       " [('wood', 'NN')],\n",
       " [('and', 'CC')],\n",
       " [('river', 'NN')],\n",
       " [('in', 'IN')],\n",
       " [('quedtion', 'NN')],\n",
       " [('were', 'VBD')],\n",
       " [('.', '.')]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag=[nltk.pos_tag([i]) for i in tokens]\n",
    "pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Enity recoginatsation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Pankaj\n",
      "[nltk_data]     kumar shah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to C:\\Users\\Pankaj kumar\n",
      "[nltk_data]     shah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Pankaj kumar shah/nltk_data'\n    - 'D:\\\\Anaconda\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Pankaj kumar shah\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words.zip/words/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Pankaj kumar shah/nltk_data'\n    - 'D:\\\\Anaconda\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Pankaj kumar shah\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-c0dd9191ec46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mne_reco\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mne_reco\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\chunk\\__init__.py\u001b[0m in \u001b[0;36mne_chunk\u001b[1;34m(tagged_tokens, binary)\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[0mchunker_pickle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_MULTICLASS_NE_CHUNKER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[0mchunker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunker_pickle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mchunker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\chunk\\named_entity.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0mEach\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtagged\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \"\"\"\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mtagged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tagged_to_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\tag\\sequential.py\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\tag\\sequential.py\u001b[0m in \u001b[0;36mtag_one\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtagger\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_taggers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\tag\\sequential.py\u001b[0m in \u001b[0;36mchoose_tag\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mchoose_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m         \u001b[1;31m# Use our feature detector to get the featureset.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 652\u001b[1;33m         \u001b[0mfeatureset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Use the classifier to pick a tag.  If a cutoff probability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\tag\\sequential.py\u001b[0m in \u001b[0;36mfeature_detector\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m         \"\"\"\n\u001b[1;32m--> 699\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feature_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\chunk\\named_entity.py\u001b[0m in \u001b[0;36m_feature_detector\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[1;34m'pos'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;34m'word'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[1;34m'en-wordlist'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_english_wordlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m             \u001b[1;34m'prevtag'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprevtag\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[1;34m'prevpos'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprevpos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\chunk\\named_entity.py\u001b[0m in \u001b[0;36m_english_wordlist\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_en_wordlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en-basic'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m             \u001b[0mwl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_en_wordlist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Pankaj kumar shah/nltk_data'\n    - 'D:\\\\Anaconda\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Pankaj kumar shah\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "ne_reco=ne_chunk(tokens)\n",
    "ne_reco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
